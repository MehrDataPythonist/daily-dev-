{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrDataPythonist/daily-dev-/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 1: Ù†ØµØ¨ Ùˆ ÙˆØ§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n"
      ],
      "metadata": {
        "id": "FzoYeRZXAr6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install ultralytics\n",
        "!pip install albumentations\n",
        "!pip install opencv-python-headless\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYA7wLMlAsGE",
        "outputId": "2b0b5058-83e4-4560-b774-6435604b10e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.41-py3-none-any.whl (792 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m793.0/793.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.41 ultralytics-thop-2.0.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (24.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 2: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„ Ø²ÛŒÙ¾ Ø§Ø² Google Drive\n"
      ],
      "metadata": {
        "id": "B1rGblV_AsRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "file_url = 'https://drive.google.com/uc?id=1Bw9lkOC1yNPie2yi7Rajx82pehi8UtVh'\n",
        "zip_path = 'dataset.zip'\n",
        "gdown.download(file_url, zip_path, quiet=False)\n",
        "\n",
        "extract_dir = 'dataset/'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "extracted_files = os.listdir(extract_dir)\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(extracted_files)}\")\n",
        "print(\"ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:\", extracted_files)\n",
        "\n",
        "internal_files = []\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    for file in files:\n",
        "        internal_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: {len(internal_files)}\")\n",
        "print(\"ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ:\", internal_files)\n",
        "\n",
        "if len(internal_files) == 0:\n",
        "    print(\"Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
        "else:\n",
        "    print(\"ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù†Ø¯.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdnEyCGaAsZa",
        "outputId": "a8146936-2b6e-4483-9287-bce19a2eaf2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Bw9lkOC1yNPie2yi7Rajx82pehi8UtVh\n",
            "From (redirected): https://drive.google.com/uc?id=1Bw9lkOC1yNPie2yi7Rajx82pehi8UtVh&confirm=t&uuid=a4013275-a3e4-42ed-9648-af439bd397a4\n",
            "To: /content/dataset.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64.6M/64.6M [00:00<00:00, 83.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: 73\n",
            "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: ['062.jpg', '026.jpg', '069.jpg', '005.jpg', '017.jpg', '010.jpg', '041.jpg', '053.jpg', '019.jpg', '025.jpg', '021.jpg', '066.jpg', '004.jpg', '018.jpg', '028.jpg', '007.jpg', '055.jpg', '036.jpg', '048.jpg', '024.jpg', '061.jpg', '031.jpg', '030.jpg', '020.jpg', '013.jpg', '014.jpg', '065.jpg', '011.jpg', '049.jpg', '044.jpg', '000.jpg', '006.jpg', '032.jpg', '039.jpg', '046.jpg', '072.jpg', '015.jpg', '052.jpg', '070.jpg', '003.jpg', '022.jpg', '038.jpg', '060.jpg', '029.jpg', '033.jpg', '057.jpg', '064.jpg', '047.jpg', '001.jpg', '009.jpg', '050.jpg', '023.jpg', '027.jpg', '063.jpg', '008.jpg', '002.jpg', '071.jpg', '037.jpg', '054.jpg', '035.jpg', '012.jpg', '059.jpg', '068.jpg', '058.jpg', '042.jpg', '040.jpg', '051.jpg', '043.jpg', '056.jpg', '034.jpg', '067.jpg', '016.jpg', '045.jpg']\n",
            "ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: 73\n",
            "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: ['dataset/062.jpg', 'dataset/026.jpg', 'dataset/069.jpg', 'dataset/005.jpg', 'dataset/017.jpg', 'dataset/010.jpg', 'dataset/041.jpg', 'dataset/053.jpg', 'dataset/019.jpg', 'dataset/025.jpg', 'dataset/021.jpg', 'dataset/066.jpg', 'dataset/004.jpg', 'dataset/018.jpg', 'dataset/028.jpg', 'dataset/007.jpg', 'dataset/055.jpg', 'dataset/036.jpg', 'dataset/048.jpg', 'dataset/024.jpg', 'dataset/061.jpg', 'dataset/031.jpg', 'dataset/030.jpg', 'dataset/020.jpg', 'dataset/013.jpg', 'dataset/014.jpg', 'dataset/065.jpg', 'dataset/011.jpg', 'dataset/049.jpg', 'dataset/044.jpg', 'dataset/000.jpg', 'dataset/006.jpg', 'dataset/032.jpg', 'dataset/039.jpg', 'dataset/046.jpg', 'dataset/072.jpg', 'dataset/015.jpg', 'dataset/052.jpg', 'dataset/070.jpg', 'dataset/003.jpg', 'dataset/022.jpg', 'dataset/038.jpg', 'dataset/060.jpg', 'dataset/029.jpg', 'dataset/033.jpg', 'dataset/057.jpg', 'dataset/064.jpg', 'dataset/047.jpg', 'dataset/001.jpg', 'dataset/009.jpg', 'dataset/050.jpg', 'dataset/023.jpg', 'dataset/027.jpg', 'dataset/063.jpg', 'dataset/008.jpg', 'dataset/002.jpg', 'dataset/071.jpg', 'dataset/037.jpg', 'dataset/054.jpg', 'dataset/035.jpg', 'dataset/012.jpg', 'dataset/059.jpg', 'dataset/068.jpg', 'dataset/058.jpg', 'dataset/042.jpg', 'dataset/040.jpg', 'dataset/051.jpg', 'dataset/043.jpg', 'dataset/056.jpg', 'dataset/034.jpg', 'dataset/067.jpg', 'dataset/016.jpg', 'dataset/045.jpg']\n",
            "ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù†Ø¯.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 3: Ø¨Ø±Ø´ Ù¾Ù„Ø§Ú©â€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ ØªØµØ§ÙˆÛŒØ± Ø¬Ø¯ÛŒØ¯\n"
      ],
      "metadata": {
        "id": "x4iGpKkUAsh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_license_plate(image, detections):\n",
        "    plates = []\n",
        "    for (x1, y1, x2, y2) in detections:\n",
        "        plate = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "        plates.append((plate, (x1, y1, x2, y2)))\n",
        "    return plates\n",
        "\n",
        "model = YOLO('yolov5s.pt')\n",
        "\n",
        "images_dir = 'dataset/'\n",
        "images = [img for img in os.listdir(images_dir) if img.endswith('.jpg')]\n",
        "\n",
        "if len(images) == 0:\n",
        "    print(\"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ±ÛŒ Ø¯Ø± Ù…Ø³ÛŒØ± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± ØªØµØ§ÙˆÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\")\n",
        "else:\n",
        "    for img in images:\n",
        "        image_path = os.path.join(images_dir, img)\n",
        "        image = cv2.imread(image_path)\n",
        "        results = model(image_path)\n",
        "\n",
        "        for result in results:\n",
        "            boxes = result.boxes.xyxy.cpu().numpy()\n",
        "            plates = crop_license_plate(image, boxes)\n",
        "\n",
        "            for i, (plate, box) in enumerate(plates):\n",
        "                plate_path = os.path.join(images_dir, f\"{os.path.splitext(img)[0]}_plate_{i}.jpg\")\n",
        "                cv2.imwrite(plate_path, plate)\n",
        "\n",
        "                label_path = os.path.join(images_dir, f\"{os.path.splitext(img)[0]}_plate_{i}.txt\")\n",
        "                with open(label_path, 'w') as f:\n",
        "                    class_id = 0\n",
        "                    x_center = (box[0] + box[2]) / 2 / image.shape[1]\n",
        "                    y_center = (box[1] + box[3]) / 2 / image.shape[0]\n",
        "                    width = (box[2] - box[0]) / image.shape[1]\n",
        "                    height = (box[3] - box[1]) / image.shape[0]\n",
        "                    f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "    print(\"Ù¾Ù„Ø§Ú©â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aid5cCTuAsnx",
        "outputId": "eb565200-778c-4ff8-8848-d08a7950099a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRO TIP ğŸ’¡ Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov5su.pt to 'yolov5su.pt'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.7M/17.7M [00:00<00:00, 368MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/dataset/062.jpg: 384x640 (no detections), 426.4ms\n",
            "Speed: 15.6ms preprocess, 426.4ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/026.jpg: 384x640 (no detections), 285.7ms\n",
            "Speed: 1.4ms preprocess, 285.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/069.jpg: 384x640 1 car, 286.8ms\n",
            "Speed: 3.1ms preprocess, 286.8ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/005.jpg: 448x640 1 person, 1 car, 389.6ms\n",
            "Speed: 2.6ms preprocess, 389.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/017.jpg: 512x640 1 truck, 630.3ms\n",
            "Speed: 3.9ms preprocess, 630.3ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/dataset/010.jpg: 448x640 1 person, 6 cars, 1 bus, 519.0ms\n",
            "Speed: 3.7ms preprocess, 519.0ms inference, 5.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/041.jpg: 416x640 5 cars, 485.3ms\n",
            "Speed: 7.3ms preprocess, 485.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/dataset/053.jpg: 384x640 3 cars, 1 truck, 471.3ms\n",
            "Speed: 3.8ms preprocess, 471.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/019.jpg: 384x640 (no detections), 291.1ms\n",
            "Speed: 1.6ms preprocess, 291.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/025.jpg: 384x640 1 car, 285.2ms\n",
            "Speed: 1.6ms preprocess, 285.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/021.jpg: 384x640 1 truck, 283.3ms\n",
            "Speed: 2.7ms preprocess, 283.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/066.jpg: 384x640 3 cars, 279.2ms\n",
            "Speed: 2.8ms preprocess, 279.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/004.jpg: 480x640 1 car, 1 truck, 358.0ms\n",
            "Speed: 2.9ms preprocess, 358.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/018.jpg: 480x640 (no detections), 348.6ms\n",
            "Speed: 1.8ms preprocess, 348.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/028.jpg: 512x640 2 cars, 381.6ms\n",
            "Speed: 3.0ms preprocess, 381.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/dataset/007.jpg: 512x640 1 car, 1 truck, 366.9ms\n",
            "Speed: 3.0ms preprocess, 366.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/dataset/055.jpg: 384x640 1 car, 284.5ms\n",
            "Speed: 2.8ms preprocess, 284.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/036.jpg: 384x640 3 cars, 268.1ms\n",
            "Speed: 2.5ms preprocess, 268.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/048.jpg: 384x640 1 car, 1 bus, 285.0ms\n",
            "Speed: 2.8ms preprocess, 285.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/024.jpg: 384x640 (no detections), 280.0ms\n",
            "Speed: 1.7ms preprocess, 280.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/061.jpg: 384x640 1 car, 1 bench, 269.6ms\n",
            "Speed: 2.6ms preprocess, 269.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/031.jpg: 448x640 2 cars, 313.4ms\n",
            "Speed: 2.3ms preprocess, 313.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/030.jpg: 448x640 4 cars, 315.0ms\n",
            "Speed: 2.7ms preprocess, 315.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/020.jpg: 384x640 1 car, 287.1ms\n",
            "Speed: 2.6ms preprocess, 287.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/013.jpg: 416x640 (no detections), 304.3ms\n",
            "Speed: 2.6ms preprocess, 304.3ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/dataset/014.jpg: 416x640 1 person, 1 car, 1 bus, 1 truck, 295.5ms\n",
            "Speed: 2.9ms preprocess, 295.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/dataset/065.jpg: 384x640 1 car, 276.3ms\n",
            "Speed: 3.0ms preprocess, 276.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/011.jpg: 640x640 1 bottle, 1 oven, 456.2ms\n",
            "Speed: 3.7ms preprocess, 456.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/dataset/049.jpg: 384x640 1 car, 1 bus, 286.7ms\n",
            "Speed: 2.8ms preprocess, 286.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/044.jpg: 640x640 1 person, 1 car, 1 bus, 443.5ms\n",
            "Speed: 2.3ms preprocess, 443.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/dataset/000.jpg: 480x640 6 persons, 4 cars, 1 handbag, 360.4ms\n",
            "Speed: 2.5ms preprocess, 360.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/006.jpg: 384x640 1 car, 385.5ms\n",
            "Speed: 2.4ms preprocess, 385.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/032.jpg: 480x640 1 train, 555.3ms\n",
            "Speed: 2.5ms preprocess, 555.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/039.jpg: 448x640 1 person, 3 cars, 1 dog, 490.1ms\n",
            "Speed: 3.5ms preprocess, 490.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/046.jpg: 384x640 1 oven, 457.3ms\n",
            "Speed: 3.9ms preprocess, 457.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/072.jpg: 384x640 1 car, 287.9ms\n",
            "Speed: 2.8ms preprocess, 287.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/015.jpg: 448x640 1 bus, 315.7ms\n",
            "Speed: 2.7ms preprocess, 315.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/052.jpg: 384x640 3 persons, 1 car, 297.8ms\n",
            "Speed: 2.7ms preprocess, 297.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/070.jpg: 384x640 1 car, 1 cell phone, 276.5ms\n",
            "Speed: 2.7ms preprocess, 276.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/003.jpg: 480x640 1 car, 1 truck, 360.5ms\n",
            "Speed: 2.8ms preprocess, 360.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/022.jpg: 384x640 1 car, 269.7ms\n",
            "Speed: 2.7ms preprocess, 269.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/038.jpg: 544x640 1 person, 6 cars, 415.3ms\n",
            "Speed: 2.9ms preprocess, 415.3ms inference, 1.7ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/dataset/060.jpg: 384x640 1 person, 2 cars, 266.2ms\n",
            "Speed: 2.8ms preprocess, 266.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/029.jpg: 448x640 1 car, 1 parking meter, 318.8ms\n",
            "Speed: 2.6ms preprocess, 318.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/033.jpg: 384x640 (no detections), 295.6ms\n",
            "Speed: 2.7ms preprocess, 295.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/057.jpg: 384x640 3 cars, 267.4ms\n",
            "Speed: 2.8ms preprocess, 267.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/064.jpg: 384x640 1 truck, 274.1ms\n",
            "Speed: 3.7ms preprocess, 274.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/047.jpg: 384x640 2 cars, 269.5ms\n",
            "Speed: 2.8ms preprocess, 269.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/001.jpg: 320x640 1 car, 252.4ms\n",
            "Speed: 2.4ms preprocess, 252.4ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "image 1/1 /content/dataset/009.jpg: 448x640 3 cars, 1 truck, 311.3ms\n",
            "Speed: 2.7ms preprocess, 311.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/050.jpg: 384x640 1 car, 287.6ms\n",
            "Speed: 2.8ms preprocess, 287.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/023.jpg: 384x640 1 car, 267.8ms\n",
            "Speed: 2.7ms preprocess, 267.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/027.jpg: 384x640 1 person, 1 car, 2 trucks, 270.6ms\n",
            "Speed: 2.5ms preprocess, 270.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/063.jpg: 352x640 1 truck, 277.5ms\n",
            "Speed: 2.3ms preprocess, 277.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/dataset/008.jpg: 512x640 1 bus, 365.7ms\n",
            "Speed: 3.0ms preprocess, 365.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/dataset/002.jpg: 480x640 (no detections), 346.3ms\n",
            "Speed: 3.2ms preprocess, 346.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/071.jpg: 384x640 1 car, 275.6ms\n",
            "Speed: 2.8ms preprocess, 275.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/037.jpg: 448x640 1 car, 1 truck, 427.9ms\n",
            "Speed: 2.9ms preprocess, 427.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/054.jpg: 384x640 4 cars, 438.4ms\n",
            "Speed: 3.7ms preprocess, 438.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/035.jpg: 416x640 3 cars, 478.4ms\n",
            "Speed: 3.3ms preprocess, 478.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/dataset/012.jpg: 448x640 2 buss, 1 truck, 531.2ms\n",
            "Speed: 3.8ms preprocess, 531.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/059.jpg: 384x640 1 car, 1 truck, 325.7ms\n",
            "Speed: 4.1ms preprocess, 325.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/068.jpg: 384x640 6 cars, 276.1ms\n",
            "Speed: 2.8ms preprocess, 276.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/058.jpg: 384x640 2 cars, 269.6ms\n",
            "Speed: 2.8ms preprocess, 269.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/042.jpg: 640x640 1 car, 1 truck, 2 traffic lights, 451.2ms\n",
            "Speed: 3.6ms preprocess, 451.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/dataset/040.jpg: 448x640 1 car, 1 truck, 332.7ms\n",
            "Speed: 2.5ms preprocess, 332.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/dataset/051.jpg: 384x640 2 cars, 281.5ms\n",
            "Speed: 2.7ms preprocess, 281.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/043.jpg: 544x640 2 cars, 393.5ms\n",
            "Speed: 3.6ms preprocess, 393.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/dataset/056.jpg: 384x640 1 car, 287.6ms\n",
            "Speed: 3.1ms preprocess, 287.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/034.jpg: 384x640 1 car, 291.4ms\n",
            "Speed: 2.8ms preprocess, 291.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/067.jpg: 384x640 1 car, 281.0ms\n",
            "Speed: 2.9ms preprocess, 281.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/dataset/016.jpg: 480x640 (no detections), 335.4ms\n",
            "Speed: 2.7ms preprocess, 335.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/dataset/045.jpg: 384x640 1 bus, 282.2ms\n",
            "Speed: 2.8ms preprocess, 282.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Ù¾Ù„Ø§Ú©â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯ Ùˆ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù†Ø¯.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##Ø³Ù„ÙˆÙ„ 5: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Training Ùˆ Validation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c4NChloIAs90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join('dataset', 'train_sample')\n",
        "val_dir = os.path.join('dataset', 'val_sample')\n",
        "\n",
        "for dir in [train_dir, val_dir]:\n",
        "    os.makedirs(os.path.join(dir, 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(dir, 'labels'), exist_ok=True)\n",
        "\n",
        "all_images = [img for img in os.listdir(images_dir) if img.endswith('.jpg')]\n",
        "random.shuffle(all_images)\n",
        "\n",
        "train_size = int(0.8 * len(all_images))\n",
        "\n",
        "train_images = all_images[:train_size]\n",
        "val_images = all_images[train_size:]\n",
        "\n",
        "def copy_files(file_list, source_img_dir, target_img_dir, source_lbl_dir, target_lbl_dir):\n",
        "    for file in file_list:\n",
        "        img_src = os.path.join(source_img_dir, file)\n",
        "        lbl_src = os.path.join(source_lbl_dir, file.replace('.jpg', '.txt'))\n",
        "        img_dst = os.path.join(target_img_dir, file)\n",
        "        lbl_dst = os.path.join(target_lbl_dir, file.replace('.jpg', '.txt'))\n",
        "        if os.path.exists(lbl_src) and img_src != img_dst and lbl_src != lbl_dst:\n",
        "            copyfile(img_src, img_dst)\n",
        "            copyfile(lbl_src, lbl_dst)\n",
        "\n",
        "copy_files(train_images, images_dir, os.path.join(train_dir, 'images'), images_dir, os.path.join(train_dir, 'labels'))\n",
        "copy_files(val_images, images_dir, os.path.join(val_dir, 'images'), images_dir, os.path.join(val_dir, 'labels'))\n",
        "\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø¢Ù…ÙˆØ²Ø´: {len(train_images)}\")\n",
        "print(f\"ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(val_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_BT7HbzAtFo",
        "outputId": "a11de62b-4b30-4860-e064-f69896e4044a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø¢Ù…ÙˆØ²Ø´: 184\n",
            "ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 7: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ YAML\n"
      ],
      "metadata": {
        "id": "jf7GTrEXBa5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_content = \"\"\"\n",
        "path: /content/dataset\n",
        "train: /content/dataset/train_sample/images\n",
        "val: /content/dataset/val_sample/images\n",
        "\n",
        "nc: 1\n",
        "names: ['plate']\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = os.path.join('/content/dataset', 'dataset_sample.yaml')\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ YAML Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46RevccLBbBu",
        "outputId": "5bebc737-be33-43c1-b3f7-2f732a923434"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ YAML Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 8: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ YOLOv5\n"
      ],
      "metadata": {
        "id": "6ntTOGrjBbK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov5su.pt')\n",
        "\n",
        "results = model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=5,\n",
        "    imgsz=640,\n",
        "    batch=4,\n",
        "    name='digit_detection_sample_small'\n",
        ")\n",
        "\n",
        "print(\"Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TkMWnFTBbS0",
        "outputId": "bc82381f-7aa6-46a6-ae22-8403de02208b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.41 ğŸš€ Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5su.pt, data=/content/dataset/dataset_sample.yaml, epochs=5, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=digit_detection_sample_small, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/digit_detection_sample_small\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 83.1MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
            " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
            " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
            " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
            " 24        [17, 20, 23]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "YOLOv5s summary: 262 layers, 9122579 parameters, 9122563 gradients, 24.0 GFLOPs\n",
            "\n",
            "Transferred 421/427 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/digit_detection_sample_small', view at http://localhost:6006/\n",
            "Freezing layer 'model.24.dfl.conv.weight'\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/train_sample/labels... 127 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<00:00, 1664.45it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/dataset/train_sample/images/042_plate_3.jpg: ignoring corrupt image/label: image size (19, 7) <10 pixels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/train_sample/labels.cache\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/val_sample/labels... 28 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:00<00:00, 2075.68it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /content/dataset/val_sample/images/039_plate_4.jpg: ignoring corrupt image/label: image size (39, 9) <10 pixels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/val_sample/labels.cache\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/digit_detection_sample_small/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/digit_detection_sample_small\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/5         0G      1.963      3.362      2.145         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [04:10<00:00,  7.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.73s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         28         28      0.374     0.0714     0.0609     0.0182\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        2/5         0G      2.043       3.11      2.141          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [04:11<00:00,  7.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.72s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         28         28     0.0779       0.25     0.0404     0.0146\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/5         0G      2.261      3.243      2.176          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [04:20<00:00,  8.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         28         28    0.00968       0.25    0.00509    0.00148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/5         0G      2.149      2.992      2.193          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [04:16<00:00,  8.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         28         28    0.00703      0.143     0.0062    0.00146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        5/5         0G      2.096      2.933      2.137         12        640:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 24/32 [03:10<01:01,  7.70s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ø³Ù„ÙˆÙ„ 9: Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ù¾Ù„Ø§Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù\n"
      ],
      "metadata": {
        "id": "yWmhkAnzBbaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('runs/detect/digit_detection_sample_small/weights/best.pt')\n",
        "\n",
        "metrics = model.val(data=yaml_path)\n",
        "\n",
        "print(\"Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„:\")\n",
        "print(metrics)\n",
        "\n",
        "results_dir = 'runs/detect/digit_detection_sample_small'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def display_image(image_path, title=''):\n",
        "    img = mpimg.imread(image_path)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "if os.path.exists(os.path.join(results_dir, 'val_batch0_labels.jpg')):\n",
        "    display_image(os.path.join(results_dir, 'val_batch0_labels.jpg'), 'Validation Batch 0 Labels')\n",
        "if os.path.exists(os.path.join(results_dir, 'val_batch0_pred.jpg')):\n",
        "    display_image(os.path.join(results_dir, 'val_batch0_pred.jpg'), 'Validation Batch 0 Predictions')\n",
        "if os.path.exists(os.path.join(results_dir, 'labels.jpg')):\n",
        "    display_image(os.path.join(results_dir, 'labels.jpg'), 'Labels')\n",
        "if os.path.exists(os.path.join(results_dir, 'confusion_matrix.png')):\n",
        "    display_image(os.path.join(results_dir, 'confusion_matrix.png'), 'Confusion Matrix')\n",
        "if os.path.exists(os.path.join(results_dir, 'PR_curve.png')):\n",
        "    display_image(os.path.join(results_dir, 'PR_curve.png'), 'Precision-Recall Curve')\n",
        "if os.path.exists(os.path.join(results_dir, 'F1_curve.png')):\n",
        "    display_image(os.path.join(results_dir, 'F1 Curve'))\n",
        "if os.path.exists(os.path.join(results_dir, 'P_curve.png')):\n",
        "    display_image(os.path.join(results_dir, 'Precision Curve'))\n",
        "if os.path.exists(os.path.join(results_dir, 'R_curve.png')):\n",
        "    display_image(os.path.join(results_dir, 'Recall Curve'))"
      ],
      "metadata": {
        "id": "5eJrogu-Bbid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsenTHRDF4Vx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}